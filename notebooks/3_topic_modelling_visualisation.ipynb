{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Determine the degree of consensus in contentious academic fields. \n",
    "\n",
    "Collect title, publication date and summaries from scholarly articles containing a certain keyword or keywords. Apply NLP models to this data to identify and categorise concepts in this field and determine statistical significance between opposing 'truths', if any. Ranking these groups according to weighted influence will prove the degree of consensus of various approaches in a given academic field.\n",
    "\n",
    "To this end the academic_consensus model has already searched the abstracts of academic papers that contain the keyword \"nutrition\" and saved it into corpus_raw.csv.  \n",
    "  \n",
    "Overview of this notebook:\n",
    "- Setup notebok environment and load data (corpus_raw.csv)\n",
    "- Review articles published per year (sklearn's Countvectorizer)\n",
    "- Create Bag Of Words (BOW) of all articles for Titles and Conclusions (nltk)\n",
    "- Create interactive BOW per year with Bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T19:18:39.508774Z",
     "start_time": "2020-01-29T19:18:30.773114Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Common\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "\n",
    "# Gensim\n",
    "from gensim.models import Phrases\n",
    "#from gensim.models.word2vec import LineSentence     - to be used when scaling\n",
    "\n",
    "# Workspace\n",
    "from IPython.core.interactiveshell import InteractiveShell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T19:18:39.578556Z",
     "start_time": "2020-01-29T19:18:39.573569Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set workspace\n",
    "sns.set()\n",
    "# Set output charackters to 110 (not 79)\n",
    "pd.options.display.width = 110\n",
    "# To give multiple cell output. Not just the last command.\n",
    "InteractiveShell.ast_node_interactivity = 'last'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and inspect corpus.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T19:18:39.669314Z",
     "start_time": "2020-01-29T19:18:39.633409Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load corpus.csv as DataFrame with parsed date format\n",
    "corpus = pd.read_csv('../data/interim/corpus_raw.csv', parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T19:18:39.780022Z",
     "start_time": "2020-01-29T19:18:39.776028Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keyword \n",
    "keyword = 'nutrition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T19:18:39.905680Z",
     "start_time": "2020-01-29T19:18:39.883740Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 794 entries, 0 to 793\n",
      "Data columns (total 3 columns):\n",
      "publication_date    794 non-null datetime64[ns, UTC]\n",
      "title               794 non-null object\n",
      "conclusions         794 non-null object\n",
      "dtypes: datetime64[ns, UTC](1), object(2)\n",
      "memory usage: 18.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>title</th>\n",
       "      <th>conclusions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-10-21 00:00:00+00:00</td>\n",
       "      <td>Psychological Determinants of Consumer Accepta...</td>\n",
       "      <td>['To the authors’ knowledge, this is the first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-03-13 00:00:00+00:00</td>\n",
       "      <td>Uncovering the Nutritional Landscape of Food</td>\n",
       "      <td>['In this study, we have developed a unique co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-06-27 00:00:00+00:00</td>\n",
       "      <td>Developing and validating a scale to measure F...</td>\n",
       "      <td>['Food and nutrition literacy scale is a valid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-18 00:00:00+00:00</td>\n",
       "      <td>Quality of nutrition services in primary healt...</td>\n",
       "      <td>['The aim of the NNS, integrating nutrition se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-10-21 00:00:00+00:00</td>\n",
       "      <td>To See or Not to See: Do Front of Pack Nutriti...</td>\n",
       "      <td>['Our work strongly supports the idea that FOP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           publication_date                                              title  \\\n",
       "0 2014-10-21 00:00:00+00:00  Psychological Determinants of Consumer Accepta...   \n",
       "1 2015-03-13 00:00:00+00:00       Uncovering the Nutritional Landscape of Food   \n",
       "2 2017-06-27 00:00:00+00:00  Developing and validating a scale to measure F...   \n",
       "3 2017-05-18 00:00:00+00:00  Quality of nutrition services in primary healt...   \n",
       "4 2015-10-21 00:00:00+00:00  To See or Not to See: Do Front of Pack Nutriti...   \n",
       "\n",
       "                                         conclusions  \n",
       "0  ['To the authors’ knowledge, this is the first...  \n",
       "1  ['In this study, we have developed a unique co...  \n",
       "2  ['Food and nutrition literacy scale is a valid...  \n",
       "3  ['The aim of the NNS, integrating nutrition se...  \n",
       "4  ['Our work strongly supports the idea that FOP...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect\n",
    "corpus.info()\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T09:57:15.035149Z",
     "start_time": "2020-01-27T09:57:14.147513Z"
    }
   },
   "source": [
    "Using Spacy to tokenise and clean each sentence in every document by removing stop words, punctuation, unnecessary white space, numbers and lemmatising each word.  \n",
    "  \n",
    "Do this for every sentence in each document and combine all documents in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T19:21:48.423676Z",
     "start_time": "2020-01-29T19:21:47.663685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set Spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T19:27:45.168895Z",
     "start_time": "2020-01-29T19:27:45.164910Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Gather all articles' 'Conclusions' into docs\n",
    "docs = corpus['conclusions']\n",
    "\n",
    "# Clean unneseccary escapes\n",
    "docs_clean = []\n",
    "for idx in range(len(docs)):\n",
    "    text = docs[idx].replace('\\'', '')\n",
    "    docs_clean.append(text.replace('\\\\n' ,''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T19:30:09.866895Z",
     "start_time": "2020-01-29T19:29:45.854160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents (articles): 794\n",
      "Total number of sentences: 6448\n",
      "Total words: 98386\n"
     ]
    }
   ],
   "source": [
    "# Use Spacy - pipe to parse each document, break it into sentences, \n",
    "# clean it (lemmatise, punctuation and white space) and combine\n",
    "# all cleaned sentences into cleaned_sents\n",
    "cleaned_sents = []\n",
    "for parsed_doc in nlp.pipe(docs_clean, n_threads=4):   \n",
    "    for sent in parsed_doc.sents:\n",
    "        type(sent)\n",
    "        cleaned_sent = [token.lemma_ for token in sent if not (token.is_stop or token.is_punct)]\n",
    "        #cleaned_sent = u' '.join(cleaned_sent)\n",
    "        cleaned_sents.append(cleaned_sent)\n",
    "        \n",
    "# Confirming output\n",
    "print('Number of documents (articles): {}'.format(len(docs_clean)))\n",
    "print('Total number of sentences: {}'.format(len(cleaned_sents)))\n",
    "print('Total words: {}'.format(sum([len(sent) for sent in cleaned_sents])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T19:30:19.469633Z",
     "start_time": "2020-01-29T19:30:18.893149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 6448\n",
      "Total words: 93268\n"
     ]
    }
   ],
   "source": [
    "# Create bigram model with Gensim\n",
    "bigram_model = Phrases(cleaned_sents)\n",
    "# Generate bigram sentences to bigram_sents\n",
    "bigram_sents = []\n",
    "[bigram_sents.append(sent) for sent in bigram_model[cleaned_sents]]\n",
    "# Confirming output\n",
    "print('Total number of sentences: {}'.format(len(bigram_sents)))\n",
    "print('Total words: {}'.format(sum([len(sent) for sent in bigram_sents])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T19:30:23.166718Z",
     "start_time": "2020-01-29T19:30:22.606216Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 6448\n",
      "Total words: 92523\n"
     ]
    }
   ],
   "source": [
    "# Create trigram model with Gensim\n",
    "trigram_model = Phrases(bigram_sents)\n",
    "# Generate trigram sentences to trigram_sents\n",
    "trigram_sents = []\n",
    "[trigram_sents.append(sent) for sent in trigram_model[bigram_sents]]\n",
    "# Confirming output\n",
    "print('Total number of sentences: {}'.format(len(trigram_sents)))\n",
    "print('Total words: {}'.format(sum([len(sent) for sent in trigram_sents])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T19:30:32.934277Z",
     "start_time": "2020-01-29T19:30:32.309910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 6448\n",
      "Total words: 92407\n"
     ]
    }
   ],
   "source": [
    "# Create quadgram model with Gensim\n",
    "qgram_model = Phrases(trigram_sents)\n",
    "# Generate quadgram sentences to quadgram_sents\n",
    "qgram_sents = []\n",
    "[qgram_sents.append(sent) for sent in qgram_model[trigram_sents]]\n",
    "# Confirming output\n",
    "print('Total number of sentences: {}'.format(len(qgram_sents)))\n",
    "print('Total words: {}'.format(sum([len(sent) for sent in qgram_sents])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T04:41:37.330496Z",
     "start_time": "2020-01-31T04:41:37.323516Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author',\n",
       " 'knowledge',\n",
       " 'study',\n",
       " 'model',\n",
       " 'factor',\n",
       " 'determine',\n",
       " 'intention',\n",
       " 'personalised_nutrition',\n",
       " 'representative',\n",
       " 'sample',\n",
       " 'european',\n",
       " 'consumer']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qgram_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T05:38:32.106274Z",
     "start_time": "2020-01-29T05:38:32.100289Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.phrases.Phrases"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T05:41:36.660762Z",
     "start_time": "2020-01-29T05:41:36.653781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'mayor', 'of', 'new_york', 'was', 'there']\n"
     ]
    }
   ],
   "source": [
    "documents = [\"the mayor of new york was there\", \"machine learning can be useful sometimes\",\"new york mayor was present\"]\n",
    "\n",
    "sentence_stream = [doc.split(\" \") for doc in documents]\n",
    "bigram = Phrases(sentence_stream, min_count=1, threshold=2)\n",
    "sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']\n",
    "print(bigram[sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T17:23:51.438288Z",
     "start_time": "2020-01-29T17:23:51.432295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'mayor', 'of', 'new', 'york', 'was', 'there'],\n",
       " ['machine', 'learning', 'can', 'be', 'useful', 'sometimes'],\n",
       " ['new', 'york', 'mayor', 'was', 'present']]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T05:50:04.799527Z",
     "start_time": "2020-01-29T05:50:04.793543Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the mayor of new york was there',\n",
       " 'machine learning can be useful sometimes',\n",
       " 'new york mayor was present']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T05:51:13.014828Z",
     "start_time": "2020-01-29T05:51:13.009866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_york', 'mayor', 'was', 'present']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[sentence_stream[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T05:24:48.152258Z",
     "start_time": "2020-01-29T05:24:48.148270Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T05:24:48.673863Z",
     "start_time": "2020-01-29T05:24:48.668876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'study develop unique computational framework systematic analysis large scale food nutritional datum network food nutrient offer global unbiased view organization nutritional connection enable discovery unexpected knowledge association food nutrient nutritional fitness gauge quality raw food accord nutritional balance appear widely disperse different food raise question origin variation food remarkably nutritional balance food solely depend characteristic individual nutrient structure intimate correlation multiple nutrient amount food underscore importance nutrient nutrient connection constitute network structure embody multiple level nutritional composition food extend analysis raw food cooked food necessary truly understand nutritional landscape food consume daily leave study consider raw food sufficient draw primary insight relatively simple system number application achievable concept present judiciously combine practical approach incorporation region specific information analysis help design strategy international food aid 24 develop strategy consider prioritization regional food base nutritional fitness suggestion locally available dietary substitute food food network fortification food bottleneck nutrient forth study implication personalized nutrition 25 People different age gender body composition health state physical activity level obtain condition specific information method simply adjust require calorie nutrient intake generate irreducible food set S1 Appendix section 4.1 result irreducible food set allow compute nutritional fitness bottleneck nutrient information particular interest individual certain dietary requirement pregnant woman recommend nutrient e.g. essential amino acid vitamin non pregnant woman 26 hand interesting check different farming method food affect food nutritional composition nutritional fitness currently data source provide information farm method S1 Dataset furthermore consideration food taste financial seasonal cultural factor analysis improve applicability method nutritional policy making nutrition education food marketing 1 27 28 aforementioned food aid personalized nutrition finally systematic approach set foundation future endeavor enhance understanding food nutrition'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:49:34.611051Z",
     "start_time": "2020-01-28T18:49:34.340772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finally , -PRON- systematic approach set the foundation for future endeavor to enhance the understanding of food and nutrition . ' ]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for sent in parsed_doc.sents:\n",
    "    parsed_sent = nlp(sent.lemma_)\n",
    "    for token in parsed_sent:\n",
    "        token.is_stop\n",
    "    \n",
    "# if not token.is_stop ...\n",
    "    \n",
    "print(parsed_sent)\n",
    "print(token.is_stop)\n",
    "#parsed_sent.is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:40:40.245233Z",
     "start_time": "2020-01-28T18:40:40.241242Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#[t for t in parsed_row.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:38:21.201457Z",
     "start_time": "2020-01-28T18:38:21.197495Z"
    }
   },
   "outputs": [],
   "source": [
    "#parsed_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = []\n",
    "doc = []\n",
    "\n",
    "for token in doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:26:30.848198Z",
     "start_time": "2020-01-28T18:26:30.832239Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "token_attr = [(token.orth_, \n",
    "               token.lemma_,\n",
    "               token.is_stop, \n",
    "               token.is_punct, \n",
    "               token.is_space, \n",
    "               token.like_num, \n",
    "               token.is_oov) for token in parsed_row]\n",
    "\n",
    "df = pd.DataFrame(token_attr, columns=['text', 'lemma', 'stop', 'punct', 'space', 'num', 'oov'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:26:38.202874Z",
     "start_time": "2020-01-28T18:26:38.199889Z"
    }
   },
   "outputs": [],
   "source": [
    "#token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:26:41.125059Z",
     "start_time": "2020-01-28T18:26:41.111097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>stop</th>\n",
       "      <th>punct</th>\n",
       "      <th>space</th>\n",
       "      <th>num</th>\n",
       "      <th>oov</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[</td>\n",
       "      <td>[</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'</td>\n",
       "      <td>'</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In</td>\n",
       "      <td>in</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>study</td>\n",
       "      <td>study</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text  lemma   stop  punct  space    num   oov\n",
       "0      [      [  False   True  False  False  True\n",
       "1      '      '  False   True  False  False  True\n",
       "2     In     in   True  False  False  False  True\n",
       "3   this   this   True  False  False  False  True\n",
       "4  study  study  False  False  False  False  True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:26:43.046339Z",
     "start_time": "2020-01-28T18:26:43.031381Z"
    }
   },
   "outputs": [],
   "source": [
    "#!time\n",
    "df_clean = df[(df['stop'] == False) & (df['punct'] == False) & (df['space'] == False) & (df['num'] == False)]['lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:26:44.152385Z",
     "start_time": "2020-01-28T18:26:44.144403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4              study\n",
       "8            develop\n",
       "10            unique\n",
       "11     computational\n",
       "12         framework\n",
       "           ...      \n",
       "530         endeavor\n",
       "532          enhance\n",
       "534    understanding\n",
       "536             food\n",
       "538        nutrition\n",
       "Name: lemma, Length: 265, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
