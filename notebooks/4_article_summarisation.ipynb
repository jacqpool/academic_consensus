{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Summarisation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Determine the degree of consensus in contentious academic fields. \n",
    "\n",
    "Collect title, publication date and summaries from scholarly articles containing a certain keyword or keywords. Apply NLP models to this data to identify and categorise concepts in this field and determine statistical significance between opposing 'truths', if any. Ranking these groups according to weighted influence will prove the degree of consensus of various approaches in a given academic field.\n",
    "\n",
    "To this end the academic_consensus model has already searched the abstracts of academic papers that contain the keyword \"nutrition\" and saved it into corpus_raw.csv.  \n",
    "  \n",
    "Overview of this notebook:\n",
    "- Setup notebok environment and load data (corpus_raw.csv)\n",
    "- Review articles published per year (sklearn's Countvectorizer)\n",
    "- Create Bag Of Words (BOW) of all articles for Titles and Conclusions (nltk)\n",
    "- Create interactive BOW per year with Bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alfrick Opidi - https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/\n",
    "Jesse JCharis - https://jcharistech.wordpress.com/2018/12/31/text-summarization-using-spacy-and-python/\n",
    "  \n",
    "Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T04:09:19.205503Z",
     "start_time": "2020-02-15T04:09:18.167243Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Common\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Workspace\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# NLP\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T04:09:19.231423Z",
     "start_time": "2020-02-15T04:09:19.222420Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:60% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set workspace\n",
    "sns.set()\n",
    "# Set output charackters to 110 (not 79)\n",
    "pd.options.display.width = 110\n",
    "# To give multiple cell output. Not just the last command.\n",
    "InteractiveShell.ast_node_interactivity = 'last'\n",
    "# Make notebook wider to fit pyLDAvis plot\n",
    "display(HTML(\"<style>.container { width:60% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and inspect corpus_raw.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T04:09:21.922424Z",
     "start_time": "2020-02-15T04:09:21.881329Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load corpus.csv as DataFrame with parsed date format\n",
    "corpus = pd.read_csv('../data/interim/corpus_raw.csv', parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T04:09:22.101765Z",
     "start_time": "2020-02-15T04:09:22.097766Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keyword \n",
    "keywords = ['nutrition', 'diet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T04:09:22.294242Z",
     "start_time": "2020-02-15T04:09:22.282271Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1530 entries, 0 to 1529\n",
      "Data columns (total 3 columns):\n",
      "publication_date    1530 non-null datetime64[ns, UTC]\n",
      "title               1530 non-null object\n",
      "conclusions         1530 non-null object\n",
      "dtypes: datetime64[ns, UTC](1), object(2)\n",
      "memory usage: 36.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>title</th>\n",
       "      <th>conclusions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-03-09 00:00:00+00:00</td>\n",
       "      <td>Pregnancy Requires Major Changes in the Qualit...</td>\n",
       "      <td>['Pregnancy tends to markedly widen the nutrit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-08-23 00:00:00+00:00</td>\n",
       "      <td>Continental-Scale Patterns Reveal Potential fo...</td>\n",
       "      <td>['In all, given the geographic patterns in die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-06-17 00:00:00+00:00</td>\n",
       "      <td>Assessing Nutritional Parameters of Brown Bear...</td>\n",
       "      <td>['Previous studies have illustrated the differ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04-17 00:00:00+00:00</td>\n",
       "      <td>The Self-Reported Clinical Practice Behaviors ...</td>\n",
       "      <td>['The present study provides a valuable insigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-10-09 00:00:00+00:00</td>\n",
       "      <td>The impact of nutritional supplement intake on...</td>\n",
       "      <td>['Our study shows that the propensity to consu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           publication_date                                              title  \\\n",
       "0 2016-03-09 00:00:00+00:00  Pregnancy Requires Major Changes in the Qualit...   \n",
       "1 2016-08-23 00:00:00+00:00  Continental-Scale Patterns Reveal Potential fo...   \n",
       "2 2015-06-17 00:00:00+00:00  Assessing Nutritional Parameters of Brown Bear...   \n",
       "3 2015-04-17 00:00:00+00:00  The Self-Reported Clinical Practice Behaviors ...   \n",
       "4 2017-10-09 00:00:00+00:00  The impact of nutritional supplement intake on...   \n",
       "\n",
       "                                         conclusions  \n",
       "0  ['Pregnancy tends to markedly widen the nutrit...  \n",
       "1  ['In all, given the geographic patterns in die...  \n",
       "2  ['Previous studies have illustrated the differ...  \n",
       "3  ['The present study provides a valuable insigh...  \n",
       "4  ['Our study shows that the propensity to consu...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect\n",
    "corpus.info()\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "- Convert DataFrame feature to list of documents (**clean_df_column**)\n",
    "- Compile new list of documents containing all search words (**search_docs_in_corpus**)\n",
    "- Convert documents to one list of all sentences, if needed (**all_docs_to_sents**)\n",
    "- Summarise each document in searched corpus (**summarise_docs**)\n",
    "- Summarise all sentences together in searched corpus (**summarise_sents**)\n",
    "- Bringing it all together in one function (**summarise_docs_or_sents**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T07:30:13.518568Z",
     "start_time": "2020-02-15T07:30:13.512583Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_df_column(df, col):\n",
    "    '''\n",
    "    For df[col] returns cleaned docs (list)\n",
    "    Remove unnecessary linespaces and combine again with punct.\n",
    "    '''\n",
    "    # Gather all articles' 'Conclusions' into docs\n",
    "    docs_corp = df[col]\n",
    "\n",
    "    # Clean unneseccary escapes\n",
    "    docs = []\n",
    "    for doc in docs_corp:\n",
    "        text = doc.replace('\\'', '')\n",
    "        text = text.replace('[', '')\n",
    "        text = text.replace(']', '')                       \n",
    "        text = text.replace('\\\\n' ,'')\n",
    "        docs.append(text)\n",
    "    # Split sentences correctly and return to list of docs with joined string of sentences\n",
    "    # Some sentences were not split between \".\" because there was no space between sentences\n",
    "    docs_temp = []\n",
    "    [docs_temp.append('. '.join(doc.split('.'))) for doc in docs]\n",
    "    \n",
    "    return docs_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T04:09:25.533298Z",
     "start_time": "2020-02-15T04:09:25.529270Z"
    }
   },
   "outputs": [],
   "source": [
    "def search_docs_in_corpus(docs, search_words):\n",
    "    ''' \n",
    "    Extract documents articles containing all of the search words.\n",
    "    docs          -> list of documents (corpus)\n",
    "    *search_words -> list of search words (str)\n",
    "    '''\n",
    "    # Find documents that contain all keywords\n",
    "    docs_hits = []\n",
    "    for doc in docs:\n",
    "        # if number of keywords found in document is the same as the \n",
    "        # number of keywords, then include that dument\n",
    "        counter = 0\n",
    "        for search_word in search_words:\n",
    "            if search_word in doc:\n",
    "                counter += 1\n",
    "        if counter == len(search_words):\n",
    "            docs_hits.append(doc)  \n",
    "    return docs_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T04:09:25.796554Z",
     "start_time": "2020-02-15T04:09:25.789574Z"
    }
   },
   "outputs": [],
   "source": [
    "def all_docs_to_sents(docs):\n",
    "    '''\n",
    "    Break up each document into sentences and combine all \n",
    "    the sentences together.\n",
    "    '''\n",
    "    # Split into sentences\n",
    "    docs_temp = []\n",
    "    [docs_temp.append(doc.split('.')) for doc in docs_hit]\n",
    "\n",
    "    # Collect all sentences into one list\n",
    "    docs_hits = []\n",
    "    for doc in docs_temp:\n",
    "        for sent in doc:\n",
    "                docs_hits.append(sent)\n",
    "            \n",
    "    # Remove double spaces\n",
    "    docs_temp = []\n",
    "    [docs_temp.append(doc.replace('  ', '')) for doc in docs_hits]\n",
    "\n",
    "    # Delete empty string lists\n",
    "    docs_sents = []\n",
    "    [docs_sents.append(doc) for doc in docs_temp if len(doc) > 1];\n",
    "    \n",
    "    return docs_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T04:09:28.230349Z",
     "start_time": "2020-02-15T04:09:28.225361Z"
    }
   },
   "outputs": [],
   "source": [
    "def summarise_docs(docs):\n",
    "    '''\n",
    "    Summarise each article into one sentence.\n",
    "    docs is a list of articles \n",
    "    '''\n",
    "    summarised = []\n",
    "    for doc in docs:\n",
    "        # Only summarize if there is more than 1 sentence (.split with one sentence outputs 2)\n",
    "        if len(doc.split('.')) > 2:\n",
    "            summarised.append(summarize(doc, word_count=50, split=True))\n",
    "        else:\n",
    "            summarised.append(doc)\n",
    "    return summarised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T04:09:28.398298Z",
     "start_time": "2020-02-15T04:09:28.394309Z"
    }
   },
   "outputs": [],
   "source": [
    "def summarise_sents(docs, ratio=0.05):\n",
    "    '''\n",
    "    Summarise all sentences.\n",
    "    '''\n",
    "    # Convert docs to all sentences\n",
    "    docs_sents = all_docs_to_sents(docs)\n",
    "    # Summarise docs\n",
    "    summarised = summarize('. '.join(docs_sents), ratio, split=True) #word_count=50,\n",
    "   \n",
    "    return summarised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T07:36:25.985051Z",
     "start_time": "2020-02-15T07:36:25.979067Z"
    }
   },
   "outputs": [],
   "source": [
    "def summarise_docs_or_sents(df, col, search_words, all_sents=False, ratio=0.05):\n",
    "    '''\n",
    "    Summarise docs or sents from all sentences containing search words.\n",
    "    df  -> The corpus dataframe\n",
    "    col -> The column to use\n",
    "    search_words -> list of words to search in the corpus\n",
    "    all_sents -> If True, collectively summarise all the sentences together \n",
    "                    in the corpus\n",
    "                 If False, summarise each document independently\n",
    "    ratio -> For use in summarise_sents\n",
    "    '''\n",
    "    # Get data from dataframe and store in docs\n",
    "    docs = clean_df_column(df, col)\n",
    "    # Find documents containing the search words\n",
    "    docs_hits = search_docs_in_corpus(docs, search_words)\n",
    "\n",
    "    if all_sents:\n",
    "        summarised = summarise_sents(docs_hits, ratio)\n",
    "        print('Summary for SENTENCES:' +'\\n')\n",
    "    else:\n",
    "        summarised = summarise_docs(docs_hits)\n",
    "        print('Summary for DOCUMENTS:' + '\\n')\n",
    "        \n",
    "    # Total number of articles\n",
    "    no_docs = len(docs)\n",
    "    # Number of ariticles containing search terms\n",
    "    no_docs_hits = len(docs_hits)\n",
    "        \n",
    "    return summarised, no_docs, no_docs_hits, search_words   # docs, docs_hits  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T04:09:32.897520Z",
     "start_time": "2020-02-15T04:09:32.793822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for each DOCUMENTS:\n",
      "\n",
      "Number of articles in corpus: 1530\n",
      "Number of articles with ['protein', 'muscle', 'exercise']: 4\n",
      "\n",
      "Sentences: \n",
      "\n",
      "Fortunately, by following the recommended diet and exercise regimes, 80% of PSSM2 WB were reported to show overall improvement as well as a significant decrease in signs of rhabdomyolysis, muscle atrophy, change in behavior and decline in performance.\n",
      "\n",
      "Our findings suggest that decrease in urinary creatinine excretion rate may appear early in CKD patients, independent of decreased protein intake assessed by urinary urea excretion, as well as of other determinants of muscle mass loss including gender, an older age, non-African origin, diabetes, lower BMI and 24-h proteinuria levels.\n",
      "\n",
      "Perhaps, by knowing the specific amino acids levels of an athlete or patient, one can use this model to predict an individualized supplement regimen appropriate for which muscle tissue recovery is desired.\n",
      "\n",
      "Mutations in components of the mitochondrial pathway for fatty acid synthesis that produce a lethal null phenotype, as observed in the knockout of mouse Lias, the terminal enzyme in the de novo pathway for protein lipoylation, are unlikely to manifest as disease in the human population.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summarise_docs_or_sents\n",
    "(summarised, no_docs, no_docs_hits, search_words) = summarise_docs_or_sents(corpus, \n",
    "                                                                            'conclusions', \n",
    "                                                                            search_words = ['protein', 'muscle', 'exercise'], \n",
    "                                                                            all_sents=False)\n",
    "# Show\n",
    "print('Number of articles in corpus: {}'.format(no_docs))\n",
    "print('Number of articles with {}: {}'.format(search_words, no_docs_hits))\n",
    "print('\\n' + 'Sentences: ' + '\\n')\n",
    "# Show sentences\n",
    "[print(sent[0] + '\\n') for sent in summarised];"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "210.535px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
